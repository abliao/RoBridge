<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation">
  <meta name="keywords" content="imitation learning mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <h1 style="font-size: 40px; font-weight: bold; text-align: center;">RoBridge: A Hierarchical Architecture Bridging Cognition and Execution <br> for General Robotic Manipulation</h1>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors" style="padding-top: 10px;">
            <span class="author-block">
              Kaidong Zhang<sup>1</sup>,
            </span>
            <span class="author-block">
              Rongtao Xu<sup>2</sup>,
            </span>
            <span class="author-block">
              Pengzhen Ren<sup>3</sup>,
            </span>
            <span class="author-block">
              Junfan Lin<sup>3</sup>,
            </span>
            <span class="author-block">
              Hefeng Wu<sup>1</sup>,
            </span>
            <span class="author-block">
              Liang Lin<sup>13</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              Xiaodan Liang <sup>13</sup><sup>*</sup>
            </span>
          </div>
          <br>

          <div class="is-size-5 publication-authors">
            <span class="font-size: 20px;"><sup>1</sup>Sun Yat-sen University,<sup>2</sup>MBZUAI,<sup>3</sup>Peng Cheng Laboratory,</span>
            <br>
            <span style="font-size: 20px;"><sup>*</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span style="font-size: 22px;">arXiv</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span style="font-size: 22px;">Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/abliao/RoBridge"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span style="font-size: 22px;">Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <!-- <div class="content has-text-justified">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <video poster="" id="calvin_0" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: -1%;">
          <source src="static/videos/intro/calvin_0.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_1" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: -0.5%;">
          <source src="static/videos/intro/calvin_1.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_2" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: 0.5%;">
          <source src="static/videos/intro/calvin_2.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_3" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: 1%;">
          <source src="static/videos/intro/calvin_3.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
           <div class="item open_the_drawer">
            <video poster="" id="open_the_drawer" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/real_exp/open_the_drawer.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_distractor">
            <video poster="" id="3task_distractor" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_distractor.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_unseen">
            <video poster="" id="3task_unseen" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_unseen.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_normal">
            <video poster="" id="3task_normal" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_normal.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div> -->

  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities.  To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Method</h2>
        <br>
        <div class="columns is-centered has-text-centered">
          <img id="multi_methods" width="80%" src="static/images/Comparison_00.png"/>
        </div>
        <!-- <video poster="" id="overview" autoplay muted loop playsinline width="70%">
            <source src="static/videos/intro/overview.mp4"
                    type="video/mp4">
        </video> -->
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            Declarative skill methods (left) directly generate specific control commands in a formulaic way, such as determining trajectories by minimizing cost.
           However, due to a lack of interaction experience with the physical world, the generated commands are often incorrect. 
           Procedural skill methods (middle) forcibly transform a vision-language model (VLM) into a robotics model using a data-driven approach, but it is not effective in dealing with unseen situations. 
           Our method, RoBridge(right), enables the VLM to generate physically intuitive representations as a symbolic bridge. This symbolic bridge is characterized by its invariance, allowing it to communicate with the underlying embodied agent in a universal manner. Meanwhile, the embodied agent continuously interacts with the physical world to gain continual skill aggregation, fully leveraging the strengths of both the VLM and reinforcement learning.
          </p>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <img id="pipeline" width="80%" src="static/images/framework_00.png"/>
        </div>
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            RoBridge adopts a three-layer architecture, consisting of a high-level cognitive planner (HCP), an invariant operable representation (IOR), and a generalist embodied agent (GEA). 
            For example, for the instruction ``Put the blocks into the corresponding shaped slots", HCP will first plan and split the task into multiple primitive actions. 
            Then, combined with the APIs composed of the foundation model, it will give IOR, which mainly includes the masked depth of the first perspective, the mask of the third perspective, the type of action, and the constraints. 
            IOR is updated by HCP at a low frequency and track-anything updates the mask at a high frequency. IOR is used as the input of GEA, and GEA performs specific actions until the task is completed.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Real Robot Experiments</h2>
        <!-- <img id="robot_result" width="80%" src="static/images/real_robot.png"> -->
        <div class="columns">
          <img id="table2" width="70%" src="static/images/table2.png" style="display: block; margin-left: auto; margin-right: auto">
        </div>
        <div class="content has-text-justified">
          <br>
          <p style="font-size: 24px;">
          </p>
          <div class="columns is-centered has-text-centered">
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -2%;">
                <source src="static/videos/pickup_00.mp4" type="video/mp4">
              </video>
              <p style="font-size: 16px;">pick up the blue block.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1.5%;">
                <source src="static/videos/sweep_00.mp4" type="video/mp4">
              </video>
              <p style="font-size: 16px;">Sweep the yellow block into pink stickers.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/press_button_00.mp4" type="video/mp4">
              </video>
              <p style="font-size: 16px;">Press the button.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/open_drawer_00.mp4" type="video/mp4">
              </video>
              <p style="font-size: 16px;">Open the drawer.</p>
            </div>

            <!-- <div class="container" style="display: flex; justify-content: center; flex-wrap: nowrap;">
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -2%;">
                  <source src="static/videos/pickup_00.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Push coffee to pink block.</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1.5%;">
                  <source src="static/videos/sweep_00.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up the juice in the front row.</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/press_button_00.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up starbucks and put it on yellow block.</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/open_drawer_00.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up starbucks and put it on yellow block.</p>
              </div>
            </div> -->
          </div>
    </div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Real-World Long-horizon Experiment</h2>
        <!-- <img id="calvin_env" width="70%" src="static/images/calvin_env.png" style="display: block; margin-left: auto; margin-right: auto"> -->
        <div class="content has-text-justified">
          <div class="container" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="70%" style="border-radius: 4%; position: relative; left: -1%;">
                <source src="static/videos/assembly_00.mp4" type="video/mp4">
              </video>
              <p style="font-size: 20px;">Put the blocks into the corresponding shaped slots.</p>
            </div>
            <div class="columns" style="flex: 1; margin: 0.5%; text-align: center;">
              <img id="table3" width="100%" src="static/images/table3.png" style="max-height: 80%; width: auto; margin-top: 20px; display: block; margin-left: auto; margin-right: 2%">
            </div>
          </div>
          <br>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Metaworld Experiments</h2>
        <div class="columns">
          <img id="table1" width="70%" src="static/images/table1.png" style="display: block; margin-left: auto; margin-right: auto">
        </div>
        <div class="columns">
          <img id="table4" width="70%" src="static/images/table4.png" style="display: block; margin-left: auto; margin-right: auto">
        </div>
      </div>
    </div>
</section>

<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Ablation Studies</h2>
        <div class="content has-text-justified">
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                GR-1 features video prediction and large-scale pretraining on video prediction.
                We perform ablation studies to study how these two factors influence the performance.
                GR-1 outperforms the variant without pre-training and the variant without pre-training and video prediction in all experiments. 
                We hypothesize that this is because the large-scale video pre-training helps GR-1 learn an accurate video prediction model which helps the robot understand what shall happen in future steps given the language instruction and previous observations. 
                And this information acts as a strong signpost for the robot to generate pertinent actions for rolling out trajectories. 
                Without pre-training, the video prediction of GR-1 w/o Video Pre-training may not be as robust.
                <br><br>
              </p>
            </div>
            <div class="columns">
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABCD_D.png" class="interpolation-image"
                  alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABC_D.png" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                <img src="static/images/ablation_10_percent_data.png" class="interpolation-image"
              alt="" style="display: block; margin-left: auto; margin-right: auto"/>
            </div>
            </div>
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                We probe into GR-1 to investigate its video prediction performance on CALVIN and real robot data.
                Results are shown in the below figure in which the images in the green boxes are the ground-truth images and those in the blue boxes are the predicted images.
                GR-1 is able to reconstruct future frames correctly on both CALVIN data and real robot data, although some details (e.g. occluded objects) are missing. 
                This video prediction signal can serve as a strong guide for action predictions.
                More results can be found in the paper.
              </p>
            </div>
            <img id="robot_result" width="65%" style="display: block;margin-left: auto; margin-right: auto;" src="static/images/fwd_pred.png">
        </div> 
      </div>
    </div>
</section> -->

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Conclusions</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            We introduce RoBridge, a novel hierarchical intelligent architecture designed to enhance robotic manipulation by bridging the gap between high-level cognitive planning and low-level physical execution. 
            The architecture integrates a high-level cognitive planner, an invariant operable representation, and a generalist embodied agent, demonstrating significant advancements in task generalization and execution robustness. 
            Through extensive experiments, RoBridge has shown superior performance and strong zero-shot generalization capabilities in unknown environments and novel tasks. 
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title" style="text-align: left;">BibTeX</h2>
        <div class="content has-text-justified">
          <pre><code style="font-size: 24px;">

          </code></pre>
        </div>
      </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p style="font-size: 24px;">
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
